{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPuqffco4Da6O00CKJVxfUz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apester/IME/blob/main/Lab16_Vanish_Grad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Setup and Data Preparation"
      ],
      "metadata": {
        "id": "dP7vTgMNeeL3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b2SZmEJedNk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "time_steps = 50\n",
        "samples = 1000\n",
        "\n",
        "X = np.random.rand(samples, time_steps, 1)\n",
        "y = (np.sum(X, axis=1) > (time_steps / 2)).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Define and Train a Simple RNN Model"
      ],
      "metadata": {
        "id": "Z6X40foofJ4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Input(shape=(time_steps, 1)))\n",
        "model.add(SimpleRNN(5, activation='tanh'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "optimizer = SGD(learning_rate=0.01)\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X, y, epochs=30, batch_size=32, verbose=1)"
      ],
      "metadata": {
        "id": "-p9aKxVDfJcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Observing the Vanishing Gradient Effect"
      ],
      "metadata": {
        "id": "eOmU6zB-fUAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define a function to get gradients\n",
        "inputs = tf.convert_to_tensor(X[:32], dtype=tf.float32)\n",
        "with tf.GradientTape() as tape:\n",
        "    preds = model(inputs)\n",
        "    loss = tf.keras.losses.binary_crossentropy(y[:32], preds)\n",
        "grads = tape.gradient(loss, model.trainable_weights)\n",
        "\n",
        "# Observe gradients\n",
        "for idx, grad in enumerate(grads):\n",
        "    print(f\"Layer {idx+1} gradients mean: {np.mean(np.abs(grad.numpy()))}\")"
      ],
      "metadata": {
        "id": "qN9cZvf8fbf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Analysis\n",
        "\n",
        "You will likely see that gradients significantly decrease (vanish) at earlier layers/time steps, making it difficult for the network to learn dependencies spanning many time steps.\n",
        "\n",
        "Discuss why this prevents Simple RNNs from effectively learning long-term dependencies."
      ],
      "metadata": {
        "id": "_xiHXE6XfiRl"
      }
    }
  ]
}
