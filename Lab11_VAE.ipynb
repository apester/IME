{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMR3WBVaj5PhEXh8YKwqrC4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apester/IME/blob/main/Lab11_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcXrtNOPUy3t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "learning_rate = 1e-3\n",
        "latent_dim = 20  # Dimensionality of the latent space\n",
        "input_dim = 28 * 28  # MNIST images are 28x28\n",
        "\n",
        "# Define the Variational Autoencoder (VAE) model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        # Encoder: Fully connected layers to parameterize mean and log variance\n",
        "        self.fc1 = nn.Linear(input_dim, 400)\n",
        "        self.fc21 = nn.Linear(400, latent_dim)  # For mean\n",
        "        self.fc22 = nn.Linear(400, latent_dim)  # For log variance\n",
        "\n",
        "        # Decoder: Fully connected layers to reconstruct the input\n",
        "        self.fc3 = nn.Linear(latent_dim, 400)\n",
        "        self.fc4 = nn.Linear(400, input_dim)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        mu = self.fc21(h1)\n",
        "        logvar = self.fc22(h1)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)  # Standard deviation\n",
        "        eps = torch.randn_like(std)    # Sample epsilon from normal distribution\n",
        "        return mu + eps * std          # Reparameterization trick\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))  # Sigmoid to bound output between 0 and 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        recon_x = self.decode(z)\n",
        "        return recon_x, mu, logvar\n",
        "\n",
        "# Loss function: Reconstruction loss + KL divergence\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    # Reconstruction loss (binary cross entropy)\n",
        "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    # KL Divergence loss\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Load MNIST dataset\n",
        "transform = transforms.ToTensor()  # Transforms images to tensor\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model, optimizer\n",
        "model = VAE(input_dim=input_dim, latent_dim=latent_dim).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training function\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.view(-1, input_dim).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Epoch {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}]  Loss: {loss.item()/len(data):.4f}\")\n",
        "    print(f\"====> Epoch {epoch} Average loss: {train_loss/len(train_loader.dataset):.4f}\")\n",
        "\n",
        "# Testing/Validation function\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data, _ in test_loader:\n",
        "            data = data.view(-1, input_dim).to(device)\n",
        "            recon, mu, logvar = model(data)\n",
        "            test_loss += loss_function(recon, data, mu, logvar).item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f\"====> Test set loss: {test_loss:.4f}\")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "\n",
        "# Visualize reconstructions from the test set\n",
        "def visualize_reconstructions():\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        data, _ = next(iter(test_loader))\n",
        "        data = data.view(-1, input_dim).to(device)\n",
        "        recon, _, _ = model(data)\n",
        "        comparison = torch.cat([data.view(-1, 1, 28, 28)[:8],\n",
        "                                recon.view(-1, 1, 28, 28)[:8]])\n",
        "        # Save the image grid\n",
        "        utils.save_image(comparison.cpu(), 'reconstructions.png', nrow=8)\n",
        "        print(\"Reconstructed images saved as reconstructions.png\")\n",
        "\n",
        "visualize_reconstructions()\n",
        "\n",
        "# Generate new samples from the latent space\n",
        "def generate_samples(num_samples=16):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Sample from standard normal distribution in latent space\n",
        "        z = torch.randn(num_samples, latent_dim).to(device)\n",
        "        samples = model.decode(z).cpu()\n",
        "        samples = samples.view(-1, 1, 28, 28)\n",
        "        utils.save_image(samples, 'generated_samples.png', nrow=4)\n",
        "        print(\"Generated sample images saved as generated_samples.png\")\n",
        "\n",
        "generate_samples()\n",
        "\n",
        "# Optionally: Display generated images using matplotlib\n",
        "def display_image(filename):\n",
        "    img = plt.imread(filename)\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Uncomment below lines to display images inline (if running in Jupyter Notebook)\n",
        "# display_image('reconstructions.png')\n",
        "# display_image('generated_samples.png')"
      ]
    }
  ]
}