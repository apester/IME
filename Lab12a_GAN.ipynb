{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPh6E0Ly5NEJwwLoCrmANuj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apester/IME/blob/main/Lab12a_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision pillow\n"
      ],
      "metadata": {
        "id": "q8WOpO_sM1zV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from PIL import Image\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "EF1Cjx1DPAVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################\n",
        "# 1. Synthetic Dataset  #\n",
        "#########################\n",
        "\n",
        "class SyntheticStyleDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A synthetic dataset for style transfer.\n",
        "    Domain A: images with a red tint.\n",
        "    Domain B: images with a blue tint (a 'styled' version of domain A).\n",
        "    Images are 64x64 with 3 channels.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_samples=100, image_size=64):\n",
        "        super().__init__()\n",
        "        self.num_samples = num_samples\n",
        "        self.image_size = image_size\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),  # scales pixel values to [0,1]\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # scale to [-1,1]\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # For reproducibility, you can seed per index if needed.\n",
        "        # Create a random noise image\n",
        "        np.random.seed(idx)\n",
        "        noise = np.random.rand(self.image_size, self.image_size, 3) * 255.0\n",
        "        noise = noise.astype(np.uint8)\n",
        "        img = Image.fromarray(noise)\n",
        "\n",
        "        # Domain A: add a red tint\n",
        "        img_a = img.copy().convert(\"RGB\")\n",
        "        img_a = np.array(img_a).astype(np.float32)\n",
        "        img_a[..., 0] = np.clip(img_a[..., 0] + 100, 0, 255)  # increase red channel\n",
        "        img_a = Image.fromarray(img_a.astype(np.uint8))\n",
        "\n",
        "        # Domain B: simulate style transfer by shifting red to blue\n",
        "        img_b = img.copy().convert(\"RGB\")\n",
        "        img_b = np.array(img_b).astype(np.float32)\n",
        "        # decrease red and increase blue channel to mimic a change in style\n",
        "        img_b[..., 0] = np.clip(img_b[..., 0] - 50, 0, 255)\n",
        "        img_b[..., 2] = np.clip(img_b[..., 2] + 50, 0, 255)\n",
        "        img_b = Image.fromarray(img_b.astype(np.uint8))\n",
        "\n",
        "        # Transform images to normalized tensors\n",
        "        img_a = self.transform(img_a)\n",
        "        img_b = self.transform(img_b)\n",
        "\n",
        "        return {\"A\": img_a, \"B\": img_b}\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = SyntheticStyleDataset(num_samples=200, image_size=64)\n",
        "batch_size = 16\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n"
      ],
      "metadata": {
        "id": "75nYiHknPJT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################\n",
        "# 2. Define Generator & Discriminator\n",
        "##################################\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple Generator network for pix2pix.\n",
        "    Architecture: Encoder (Conv layers) -> Decoder (ConvTranspose layers)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_nc=3, output_nc=3, n_features=64):\n",
        "        super(Generator, self).__init__()\n",
        "        # Encoder layers\n",
        "        self.down1 = nn.Sequential(\n",
        "            nn.Conv2d(input_nc, n_features, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.down2 = nn.Sequential(\n",
        "            nn.Conv2d(n_features, n_features * 2, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(n_features * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.down3 = nn.Sequential(\n",
        "            nn.Conv2d(n_features * 2, n_features * 4, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(n_features * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "\n",
        "        # Decoder layers\n",
        "        self.up1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(n_features * 4, n_features * 2, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(n_features * 2),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.up2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(n_features * 2, n_features, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(n_features),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.up3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(n_features, output_nc, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        d1 = self.down1(x)\n",
        "        d2 = self.down2(d1)\n",
        "        d3 = self.down3(d2)\n",
        "        u1 = self.up1(d3)\n",
        "        u2 = self.up2(u1)\n",
        "        u3 = self.up3(u2)\n",
        "        return u3\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    A PatchGAN discriminator for pix2pix.\n",
        "    It takes a concatenation of the input and the output (generated or real) image.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_nc=6, n_features=64):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(input_nc, n_features, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(n_features, n_features * 2, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(n_features * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(n_features * 2, n_features * 4, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(n_features * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(n_features * 4, 1, kernel_size=4, stride=1, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input_img, target_img):\n",
        "        # Concatenate input and target images along the channel dimension\n",
        "        x = torch.cat([input_img, target_img], dim=1)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        return x\n",
        "\n",
        "# Initialize models\n",
        "netG = Generator().to(device)\n",
        "netD = Discriminator().to(device)\n",
        "\n",
        "netG.apply(weights_init)\n",
        "netD.apply(weights_init)"
      ],
      "metadata": {
        "id": "Ls_LtGHoPTjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################\n",
        "# 3. Losses, Optimizers, etc.\n",
        "##############################\n",
        "\n",
        "criterion_GAN = nn.BCELoss()\n",
        "criterion_L1 = nn.L1Loss()\n",
        "\n",
        "# Hyperparameters\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "n_epochs = 5\n",
        "lambda_L1 = 100\n",
        "\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n"
      ],
      "metadata": {
        "id": "9SeIcNX2QHfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################\n",
        "# 4. Training Loop for Pix2Pix Style GAN\n",
        "##########################################\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "for epoch in range(n_epochs):\n",
        "    epoch_start = time.time()\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        # Get data and move to device\n",
        "        real_A = batch[\"A\"].to(device)  # input image (red-tinted)\n",
        "        real_B = batch[\"B\"].to(device)  # target image (blue-tinted)\n",
        "\n",
        "        # Create real and fake labels\n",
        "        # Here real_label=1 and fake_label=0\n",
        "        real_label = torch.full((real_A.size(0), 1, 6, 6), 1.0, device=device)\n",
        "        fake_label = torch.full((real_A.size(0), 1, 6, 6), 0.0, device=device)\n",
        "\n",
        "        ############################\n",
        "        # Update Generator: G\n",
        "        ############################\n",
        "        optimizerG.zero_grad()\n",
        "\n",
        "        # Generate fake images from input A\n",
        "        fake_B = netG(real_A)\n",
        "        # Evaluate the discriminator with fake output (detach not used here because we need gradients to flow back into G)\n",
        "        pred_fake = netD(real_A, fake_B)\n",
        "        loss_GAN = criterion_GAN(pred_fake, real_label)\n",
        "        # L1 loss between the generated image and the real image B\n",
        "        loss_L1 = criterion_L1(fake_B, real_B)\n",
        "        # Total generator loss\n",
        "        loss_G = loss_GAN + lambda_L1 * loss_L1\n",
        "\n",
        "        loss_G.backward()\n",
        "        optimizerG.step()\n",
        "\n",
        "        ############################\n",
        "        # Update Discriminator: D\n",
        "        ############################\n",
        "        optimizerD.zero_grad()\n",
        "        # Compute loss with real images\n",
        "        pred_real = netD(real_A, real_B)\n",
        "        loss_D_real = criterion_GAN(pred_real, real_label)\n",
        "        # Compute loss with fake images (detach so gradients donâ€™t flow to G)\n",
        "        pred_fake = netD(real_A, fake_B.detach())\n",
        "        loss_D_fake = criterion_GAN(pred_fake, fake_label)\n",
        "        # Total discriminator loss\n",
        "        loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
        "\n",
        "        loss_D.backward()\n",
        "        optimizerD.step()\n",
        "\n",
        "        # Optionally, print statistics every few iterations\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{n_epochs}] Batch [{i}/{len(dataloader)}] \"\n",
        "                  f\"Loss_G: {loss_G.item():.4f} Loss_D: {loss_D.item():.4f}\")\n",
        "\n",
        "    epoch_time = time.time() - epoch_start\n",
        "    print(f\"Epoch {epoch+1} completed in {epoch_time:.2f} sec.\")\n"
      ],
      "metadata": {
        "id": "9FdGB3iwQsol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################\n",
        "# 5. Save Outputs and Model #\n",
        "#############################\n",
        "\n",
        "os.makedirs(\"output\", exist_ok=True)\n",
        "# Save a batch of input, output and target images for visual comparison\n",
        "sample_batch = next(iter(dataloader))\n",
        "real_A_sample = sample_batch[\"A\"].to(device)\n",
        "real_B_sample = sample_batch[\"B\"].to(device)\n",
        "with torch.no_grad():\n",
        "    fake_B_sample = netG(real_A_sample)\n",
        "\n",
        "# Concatenate images: input (A), generated (fake B), and target (B)\n",
        "combined = torch.cat([real_A_sample, fake_B_sample, real_B_sample], dim=0)\n",
        "vutils.save_image(combined, \"output/sample_comparison.png\", nrow=batch_size, normalize=True)\n",
        "print(\"Saved sample image comparison to output/sample_comparison.png\")\n",
        "\n",
        "# Save the generator model checkpoint\n",
        "torch.save(netG.state_dict(), \"output/netG.pth\")\n",
        "print(\"Saved generator model weights to output/netG.pth\")\n"
      ],
      "metadata": {
        "id": "kx-1lOpBQ5pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krad4Y1CMzrK"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}